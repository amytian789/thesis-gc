iter <- 50
set.seed(10)
X <- rbind(MASS::mvrnorm(100, rep(0,2), diag(2)),
MASS::mvrnorm(100, c(0.5,0), diag(2)))
y <- rep(0,200)
for (i in 1:200) {
if (X[i,1] > 0 & X[i,2] > 0) y[i] = 1
}
y <- factor(y)
# Randomly set up the unlabeled data
set.seed(10)
y_unlabeled <- y
y_unlabeled[sample(1:200,180)] <- NA
################################## Overall classifier and return methods
# Generic classifier method: X and y contain labeled points
# This is the MAIN overall classifier that will train on the data once the AL selection is completed
classifier_method <- function(X, y, ...) {
MASS::lda(X, y)
}
# Generic classifier method prediction: X contain all points to predict
# This is the MAIN overall classifier that will train on the data once the AL selection is completed
classifier_predict <- function(classifier, X, ...) {
stats::predict(classifier, X)$class
}
# QBC classifier method: X and y contain labeled points
qbc_majority <- function(X, y, committee, ...) {
tout <- vector("list",length(committee))
for (i in 1:length(committee)){
tout[[i]] <- caret::train(X,y,committee[i])
}
tout
}
# Generic error ratio: X contain all points. y are known labels (unknown to the learning algorithm)
return_method <- function(classifier, X, y, ...) {
p <- stats::predict(classifier, X)$class
length(which(p != y))/length(y)
}
# QBC error ratio: X contain all points. y are known labels (unknown to the learning algorithm)
qbc_m_return <- function(tout, X, y, committee, ...) {
p <- vector("list",length(committee))
for (i in 1:length(committee)) {
p[[i]] <- predict(tout[[i]],newdata=X)
}
# Aggregate prediction
ap <- rep(0,length(y))
for (i in 1:length(y)){
temp <- as.numeric(as.character(p[[1]][i]))
for (j in 2:length(committee)){
temp <- c(temp,as.numeric(as.character(p[[j]][i])))
}
# error checking if a value doesn't appear at all
if (is.na(as.numeric(sort(table(temp),decreasing=TRUE)[2]))) {
ap[i] <- as.numeric(names(sort(table(temp),decreasing=TRUE)[1]))
} else {
# pick one at random if there is a tie
if (as.numeric(sort(table(temp),decreasing=TRUE)[1]) == as.numeric(sort(table(temp),decreasing=TRUE)[2])){
temp <- c(0,1)
ap[i] <- sample(temp,1)
} else {
# Otherwise, insert the first one
ap[i] <- as.numeric(names(sort(table(temp),decreasing=TRUE)[1]))
}
}
}
length(which(ap != y))/length(y)
}
###################################
# The number of random unlabeled points it "streams" to the AL / RS method
# n = 0 indicates that the AL should sample from all data points
n <- 15
k <- 1
#initialize
us_lda_results <- rep(0, iter)
qbc_results <- rep(0, iter)
qbb_results <- rep(0, iter)
cluster_results <- rep(0, iter)
random_results <- rep(0, iter)
# classifier performance given all data is labeled
pred <- classifier_method(X,y)
perf_results <- rep(return_method(pred,X,y),iter)
#run the engine (average over k = 1000 random samples)
for (i in 1:k){
set.seed(10)
us_lda_results <- us_lda_results +
AL_engine(X=X, y=y, y_unlabeled=y_unlabeled, al_method = "us", classifier_method = classifier_method,
return_method = return_method, iter = iter, n = n,
classifier = "lda")
### To change the committee, you must set it in the AL_engine
set.seed(10)
qbc_results <- qbc_results +
AL_engine(X=X, y=y, y_unlabeled=y_unlabeled, al_method = "qbc", classifier_method = qbc_majority,
return_method = qbc_m_return, iter = iter, n = n,
dis = "vote_entropy", pt = 0.75)
set.seed(10)
qbb_results <- qbb_results +
AL_engine(X=X, y=y, y_unlabeled=y_unlabeled, al_method = "qbb", classifier_method = classifier_method,
return_method = return_method, iter = iter, n = n,
classifier_train=classifier_method, classifier_predict=classifier_predict,
num_class=10, r=0.75, dis = "vote_entropy")
set.seed(10)
cluster_results <- cluster_results +
AL_engine(X=X, y=y, y_unlabeled=y_unlabeled, al_method = "cluster", classifier_method = classifier_method,
return_method = return_method, iter = iter,
n = n, dis = "euclidean")
set.seed(10)
random_results <- random_results +
AL_engine(X, y, y_unlabeled, al_method = "rs", classifier_method, return_method, iter, n)
print(c("Trial ",i,"complete"))
}
# Average
us_lda_vec <- us_lda_results / k
random_vec <- random_results / k
qbc_vec <- qbc_results / k
qbb_vec <- qbb_results / k
cluster_vec <- cluster_results / k
###################################
pdf(file=paste0("C:/Users/amyti/Documents/Amy - COLLEGE/THESIS/thesis-al/results/lda_", date, ".PDF"),
height = 6, width = 10)
#plot
ymax <- max(c(us_lda_vec, random_vec, qbc_vec,cluster_vec))
graphics::plot(1:iter, random_vec, ylim = c(0, ymax), lwd = 2, type = "l",
main="AL Error Ratio with LDA Classifier", xlab="Iterations", ylab="Error", col = "red")
graphics::lines(1:iter, us_lda_vec, lwd = 2, col = "black")
graphics::lines(1:iter, qbc_vec, lwd = 2, col = "blue")
graphics::lines(1:iter, qbb_vec, lwd = 2, col = "purple")
graphics::lines(1:iter, cluster_vec, lwd = 2, col = "orange")
graphics::lines(1:iter, perf_results, lwd = 2, col = "green")
graphics::legend(x="bottomleft",lwd=2,cex = 0.75,legend=
c("Random Sampling","Uncertainty Sampling","Query by Committee","Query by Bagging","Min-Max Clustering",
"Given all data with labels"),
col=c("red","black","blue","purple","orange","green"))
graphics.off()
save.image(file = paste0("C:/Users/amyti/Documents/Amy - COLLEGE/THESIS/thesis-al/results/lda_", date, ".RData"))
source("C:/Users/amyti/Documents/Amy - COLLEGE/THESIS/thesis-al/main/AL_engine.R")
################################
# set up the data
date <- Sys.Date()
iter <- 50
set.seed(10)
X <- rbind(MASS::mvrnorm(100, rep(0,2), diag(2)),
MASS::mvrnorm(100, c(0.5,0), diag(2)))
y <- rep(0,200)
for (i in 1:200) {
if (X[i,1] > 0 & X[i,2] > 0) y[i] = 1
}
y <- factor(y)
# Randomly set up the unlabeled data
set.seed(10)
y_unlabeled <- y
y_unlabeled[sample(1:200,180)] <- NA
################################## Overall classifier and return methods
# Generic classifier method: X and y contain labeled points
# This is the MAIN overall classifier that will train on the data once the AL selection is completed
classifier_method <- function(X, y, ...) {
MASS::lda(X, y)
}
# Generic classifier method prediction: X contain all points to predict
# This is the MAIN overall classifier that will train on the data once the AL selection is completed
classifier_predict <- function(classifier, X, ...) {
stats::predict(classifier, X)$class
}
# QBC classifier method: X and y contain labeled points
qbc_majority <- function(X, y, committee, ...) {
tout <- vector("list",length(committee))
for (i in 1:length(committee)){
tout[[i]] <- caret::train(X,y,committee[i])
}
tout
}
# Generic error ratio: X contain all points. y are known labels (unknown to the learning algorithm)
return_method <- function(classifier, X, y, ...) {
p <- stats::predict(classifier, X)$class
length(which(p != y))/length(y)
}
# QBC error ratio: X contain all points. y are known labels (unknown to the learning algorithm)
qbc_m_return <- function(tout, X, y, committee, ...) {
p <- vector("list",length(committee))
for (i in 1:length(committee)) {
p[[i]] <- predict(tout[[i]],newdata=X)
}
# Aggregate prediction
ap <- rep(0,length(y))
for (i in 1:length(y)){
temp <- as.numeric(as.character(p[[1]][i]))
for (j in 2:length(committee)){
temp <- c(temp,as.numeric(as.character(p[[j]][i])))
}
# error checking if a value doesn't appear at all
if (is.na(as.numeric(sort(table(temp),decreasing=TRUE)[2]))) {
ap[i] <- as.numeric(names(sort(table(temp),decreasing=TRUE)[1]))
} else {
# pick one at random if there is a tie
if (as.numeric(sort(table(temp),decreasing=TRUE)[1]) == as.numeric(sort(table(temp),decreasing=TRUE)[2])){
temp <- c(0,1)
ap[i] <- sample(temp,1)
} else {
# Otherwise, insert the first one
ap[i] <- as.numeric(names(sort(table(temp),decreasing=TRUE)[1]))
}
}
}
length(which(ap != y))/length(y)
}
###################################
# The number of random unlabeled points it "streams" to the AL / RS method
# n = 0 indicates that the AL should sample from all data points
n <- 15
k <- 1
#initialize
us_lda_results <- rep(0, iter)
qbc_results <- rep(0, iter)
qbb_results <- rep(0, iter)
cluster_results <- rep(0, iter)
random_results <- rep(0, iter)
# classifier performance given all data is labeled
pred <- classifier_method(X,y)
perf_results <- rep(return_method(pred,X,y),iter)
#run the engine (average over k = 1000 random samples)
for (i in 1:k){
set.seed(10)
us_lda_results <- us_lda_results +
AL_engine(X=X, y=y, y_unlabeled=y_unlabeled, al_method = "us", classifier_method = classifier_method,
return_method = return_method, iter = iter, n = n,
classifier = "lda")
### To change the committee, you must set it in the AL_engine
set.seed(10)
qbc_results <- qbc_results +
AL_engine(X=X, y=y, y_unlabeled=y_unlabeled, al_method = "qbc", classifier_method = qbc_majority,
return_method = qbc_m_return, iter = iter, n = n,
dis = "vote_entropy", pt = 0.75)
set.seed(10)
qbb_results <- qbb_results +
AL_engine(X=X, y=y, y_unlabeled=y_unlabeled, al_method = "qbb", classifier_method = classifier_method,
return_method = return_method, iter = iter, n = n,
classifier_train=classifier_method, classifier_predict=classifier_predict,
num_class=10, r=0.75, dis = "vote_entropy")
set.seed(10)
cluster_results <- cluster_results +
AL_engine(X=X, y=y, y_unlabeled=y_unlabeled, al_method = "cluster", classifier_method = classifier_method,
return_method = return_method, iter = iter,
n = n, dis = "euclidean")
set.seed(10)
random_results <- random_results +
AL_engine(X, y, y_unlabeled, al_method = "rs", classifier_method, return_method, iter, n)
print(c("Trial ",i,"complete"))
}
# Average
us_lda_vec <- us_lda_results / k
random_vec <- random_results / k
qbc_vec <- qbc_results / k
qbb_vec <- qbb_results / k
cluster_vec <- cluster_results / k
###################################
pdf(file=paste0("C:/Users/amyti/Documents/Amy - COLLEGE/THESIS/thesis-al/results/lda_", date, ".PDF"),
height = 6, width = 10)
#plot
ymax <- max(c(us_lda_vec, random_vec, qbc_vec,cluster_vec))
graphics::plot(1:iter, random_vec, ylim = c(0, ymax), lwd = 2, type = "l",
main="AL Error Ratio with LDA Classifier", xlab="Iterations", ylab="Error", col = "red")
graphics::lines(1:iter, us_lda_vec, lwd = 2, col = "black")
graphics::lines(1:iter, qbc_vec, lwd = 2, col = "blue")
graphics::lines(1:iter, qbb_vec, lwd = 2, col = "purple")
graphics::lines(1:iter, cluster_vec, lwd = 2, col = "orange")
graphics::lines(1:iter, perf_results, lwd = 2, col = "green")
graphics::legend(x="bottomleft",lwd=2,cex = 0.75,legend=
c("Random Sampling","Uncertainty Sampling","Query by Committee","Query by Bagging","Min-Max Clustering",
"Given all data with labels"),
col=c("red","black","blue","purple","orange","green"))
graphics.off()
save.image(file = paste0("C:/Users/amyti/Documents/Amy - COLLEGE/THESIS/thesis-al/results/lda_", date, ".RData"))
qbb_results
pdf(file=paste0("C:/Users/amyti/Documents/Amy - COLLEGE/THESIS/thesis-al/results/lda_", date, ".PDF"),
height = 6, width = 10)
#plot
ymax <- max(c(us_lda_vec, random_vec, qbc_vec,cluster_vec))
graphics::plot(1:iter, perf_results, ylim = c(0, ymax), lwd = 2, type = "l",
main="AL Error Ratio with LDA Classifier", xlab="Iterations", ylab="Error", col = "green")
graphics::lines(1:iter, random_vec, lwd = 2, col = "red")
graphics::lines(1:iter, us_lda_vec, lwd = 2, col = "black")
graphics::lines(1:iter, qbc_vec, lwd = 2, col = "blue")
graphics::lines(1:iter, qbb_vec, lwd = 2, col = "purple")
graphics::lines(1:iter, cluster_vec, lwd = 2, col = "orange")
graphics::legend(x="bottomleft",lwd=2,cex = 0.75,legend=
c("Random Sampling","Uncertainty Sampling","Query by Committee","Query by Bagging","Min-Max Clustering",
"Given all data with labels"),
col=c("red","black","blue","purple","orange","green"))
graphics.off()
save.image(file = paste0("C:/Users/amyti/Documents/Amy - COLLEGE/THESIS/thesis-al/results/lda_", date, ".RData"))
pdf(file=paste0("C:/Users/amyti/Documents/Amy - COLLEGE/THESIS/thesis-al/results/lda_", date, ".PDF"),
height = 6, width = 10)
#plot
ymax <- max(c(us_lda_vec, random_vec, qbc_vec,cluster_vec))
graphics::plot(1:iter, perf_results, ylim = c(0, ymax), lwd = 2, type = "l",
main="AL Error Ratio with LDA Classifier", xlab="Iterations", ylab="Error", col = "green")
graphics::lines(1:iter, random_vec, lwd = 2, col = "red")
graphics::lines(1:iter, us_lda_vec, lwd = 2, col = "black")
graphics::lines(1:iter, qbc_vec, lwd = 2, col = "blue")
graphics::lines(1:iter, qbb_vec, lwd = 2, col = "purple")
graphics::lines(1:iter, cluster_vec, lwd = 2, col = "orange")
graphics::legend(x="bottomleft",lwd=2,cex = 0.75,legend=
c("Random Sampling","Uncertainty Sampling","Query by Committee","Query by Bagging","Min-Max Clustering",
"Given all data with labels"),
col=c("red","black","blue","purple","orange","green"))
graphics.off()
save.image(file = paste0("C:/Users/amyti/Documents/Amy - COLLEGE/THESIS/thesis-al/results/lda_", date, ".RData"))
help(plot)
###################################
pdf(file=paste0("C:/Users/amyti/Documents/Amy - COLLEGE/THESIS/thesis-al/results/lda_", date, ".PDF"),
height = 6, width = 10)
#plot
ymax <- max(c(us_lda_vec, random_vec, qbc_vec,cluster_vec))
graphics::plot(1:iter, perf_results, ylim = c(0, ymax), lwd = 2, type = "l",
main="AL Error Ratio with LDA Classifier", xlab="Iterations", ylab="Error", col = "green")
graphics::lines(1:iter, random_vec, lwd = 2, col = "red")
graphics::lines(1:iter, us_lda_vec, lwd = 2, col = "black")
graphics::lines(1:iter, qbc_vec, lwd = 2, col = "blue")
graphics::lines(1:iter, qbb_vec, lwd = 2, col = "darkturquoise")
graphics::lines(1:iter, cluster_vec, lwd = 2, col = "orange")
graphics::legend(x="bottomleft",lwd=2,cex = 0.75,legend=
c("Random Sampling","Uncertainty Sampling","Query by Committee","Query by Bagging","Min-Max Clustering",
"Given all data with labels"),
col=c("red","black","blue","darkturquoise","orange","green"))
graphics.off()
save.image(file = paste0("C:/Users/amyti/Documents/Amy - COLLEGE/THESIS/thesis-al/results/lda_", date, ".RData"))
load("C:/Users/amyti/Documents/Amy - COLLEGE/THESIS/thesis-al/results/rs_2017-02-15.RData")
setwd("C:/Users/amyti/Documents/Amy - COLLEGE/THESIS/thesis-gc/")
source("main/GC_engine.R")
source("main/dist_engine.R")
library(xlsx)
library(igraph)
# Read in data.
g <- 2 # number of sheets in file. Must be an even number
data <- vector("list",length(g))
for (i in 1:g) {
# First number = number of nodes in graph
data[[i]] <- as.vector(t( xlsx::read.xlsx("data.xlsx", sheetIndex = i, header = FALSE) ))[-1]
}
for (i in seq(1,g,2)) {
# g1 and g2 must have the same number of nodes!
if (data[[i]][1] != data[[i+1]][1]) {
stop(paste("g1 and g2 must have the same # of nodes. Error in sheets",i,"and",i+1))
}
# create g1 and g2
g1 <- igraph::make_empty_graph(n = data[[i]][1], directed = FALSE)
g1 <- igraph::add_edges(g1, data[[i]][-1])
g2 <- igraph::make_empty_graph(n = data[[i+1]][1], directed = FALSE)
g2 <- igraph::add_edges(g2, data[[i+1]][-1])
# call the engine to compute difference between g1 and g2
}
source("GC_engine.R")
source("dist_engine.R")
gSumm <- c("cen","ast")
gSumm
names(gSumm)
names(gSumm)[i]
names(gSumm) <- rep("",length(gSumm))
gSumm
diff <- rep(0,length(gSumm))
gSumm <- c("cen","ast")
ggSumm
gSumm
names(diff) <- rep("",length(gSumm))
i <- 1
diff
names(diff)[1]
names(diff)[i] <- gSumm[i]
diff
i
i <- i +1
names(diff)[i] <- gSumm[i]
diff
centr_clo_tmax(g2)
centr_clo(g2)
closeness(g2)
distances(g2, v = 1, to = 2)
g2
distances(g2, v = 1, to = 3)
distances(g2, v = 1, to = 6)
centralize(closeness(g2))
closeness(g2)
centralize(closeness(g2))
centr_clo(g2)
centralize(centr_clo(g2)$res)
> centr_clo(g2)
centr_clo(g2)
centr_clo(g2, mode = "al")
centr_clo(g2, mode = "all")
centr_clo_tmax(g2)
closeness(g2, mode="all")
closeness(g2, mode="all", normalized  = FALSE)
centr_clo(g2, mode="all", normalized  = FALSE)
centr_clo(g2, mode="all", normalized  = TRUE)
closeness(g2)
cc <- closeness(g2)
cc
max(cc) - c[1]
max(cc) - cc[1]
max(cc) - cc[2]
max(cc) - cc[3]
degree(g2)
centr_deg(g2)
centr_degree(g2)
estimate_closeness(g2)
estimate_closeness(g2, cutoff = 5)
closeness(g2, normalized = TRUE)
closeness(g2, normalized = FALSE)
centr_clo(g2,normalized = TRUE)
centr_clo(g2)
centr_clo(g2, normalized = FALSE)
centr_degree(g2, normalized = FALSE)
centr_degree(g2, normalized = TRUE)
centr_clo(g2)
ccentr_degree(g2)
centr_degree(g2)
centr_betw(g2)
centr_eigen(g2)
assortativity_degree
assortativity_degree(g2)
assortativity(g2)
abs(1-5)
distances(g1)
distances(g2)
distances(g1)[which(distances(g1)==Inf)]
distances(g1)[which(distances(g1)==Inf)] <- 0
distances(g1)[which(distances(g1)==Inf)] <- 0
help(replace)
replace(distances(g1)[which(distances(g1)==Inf)],0)
replace(distances(g1)[which(distances(g1)==Inf)],distances(g1)[which(distances(g1)==Inf)]==1,0)
replace(distances(g1)[which(distances(g1)==Inf)],0)
test <- distances(g1)
test
test[test == Inf] <- 0
test
(distances(g1)[distances(g1)==Inf)] <- 0
distances(g1)[distances(g1)==Inf)] <- 0
distances(g1)[distances(g1)==Inf] <- 0
dist_euc(1,2)
dist_euc(1,5)
dist_abs(1,5)
source("dist_engine.R")
source("dist_engine.R")
dist_euc(1,5)
dist_abs(1,5)
dist_euc(c(1,2),c(5,5))
dist_abs(c(1,2),c(5,5))
source("dist_engine.R")
dist_abs(c(1,2),c(5,5))
help(attr)
test
attr(test,"upper")
attr(test,"upper") <- TRUE
test
m <- matrix(rpois(50,5), nrow=5)
m2 <- dist(m)
m2
attr(m2,"upper") <- TRUE
m2
attr(m2,"Upper") <- TRUE
m2
attr(test,"Upper") <- TRUE
test
attr(test,"Upper") <- FALSE
test
A <- matrix(1:16, 4)
A
d <- row(A) - col(A)
d
split(A, d)
upper.tri(test)
test[upper.tri(test)]
test <- distances(g2)
test[test == Inf] <- 0
test
test[upper.tri(test)]
a <- distances(g1)
b <- distances(g2)
# change from matrix -> vector for distance computation (ordered by column)
# keep only 1 side of the matrix (both sides are the same)
# don't keep the values in the middle (since it's the same node)
a[a == Inf] <- 0
a <- a[upper.tri(a)]
b[b == Inf] <- 0
b <- b[upper.tri(b)]
a
b
dist <- "Euclidean"
diff[i] <- dist_engine(a,b,dist)
diff
dist_engine(a,b,dist)
is.graph(g2)
is_graph(g2)
is_graphical(g2)
is_igraph(g2)
vcount(g2)
vcount(g1)
install.packages("clustereval")
install.packages("clusteval")
library(clusteval)
data1 <- c(1,1,1,2,2,2,3,3,3)
data2 <- c(1,1,1,1,1,1,3,3,3)
cluster_similarity(data1,data2)
cluster_similarity(data1,data1)
data1 <- c(1,1,1,1,1,2,2,2,2,2,3,3,3,3,3)
data2 <- c(1,1,1,1,1,1,1,1,1,1,3,3,3,3,3)
cluster_similarity(data1,data1)
cluster_similarity(data1,data1)
cluster_similarity(data1,data2)
data1 <- c(1,1,1,1,2,2,2,2,3,3,3,3,4,4,4)
data2 <- c(1,1,1,1,2,2,2,2,3,3,3,3,3,3,3)
cluster_similarity(data1,data2)
data2 <- c(1,1,1,1,3,3,3,3,3,3,3,3,3,3,3)
cluster_similarity(data1,data2)
