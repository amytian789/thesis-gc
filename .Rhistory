return_method = return_method, iter = iter, n = n,
classifier = "lda")
### To change the committee, you must set it in the AL_engine
set.seed(10)
qbc_results <- qbc_results +
AL_engine(X=X, y=y, y_unlabeled=y_unlabeled, al_method = "qbc", classifier_method = qbc_majority,
return_method = qbc_m_return, iter = iter, n = n,
dis = "vote_entropy", pt = 0.75)
set.seed(10)
qbb_results <- qbb_results +
AL_engine(X=X, y=y, y_unlabeled=y_unlabeled, al_method = "qbb", classifier_method = classifier_method,
return_method = return_method, iter = iter, n = n,
classifier_train=classifier_method, classifier_predict=classifier_predict,
num_class=10, r=0.75, dis = "vote_entropy")
set.seed(10)
cluster_results <- cluster_results +
AL_engine(X=X, y=y, y_unlabeled=y_unlabeled, al_method = "cluster", classifier_method = classifier_method,
return_method = return_method, iter = iter,
n = n, dis = "euclidean")
set.seed(10)
random_results <- random_results +
AL_engine(X, y, y_unlabeled, al_method = "rs", classifier_method, return_method, iter, n)
print(c("Trial ",i,"complete"))
}
# Average
us_lda_vec <- us_lda_results / k
random_vec <- random_results / k
qbc_vec <- qbc_results / k
qbb_vec <- qbb_results / k
cluster_vec <- cluster_results / k
###################################
pdf(file=paste0("C:/Users/amyti/Documents/Amy - COLLEGE/THESIS/thesis-al/results/lda_", date, ".PDF"),
height = 6, width = 10)
#plot
ymax <- max(c(us_lda_vec, random_vec, qbc_vec,cluster_vec))
graphics::plot(1:iter, random_vec, ylim = c(0, ymax), lwd = 2, type = "l",
main="AL Error Ratio with LDA Classifier", xlab="Iterations", ylab="Error", col = "red")
graphics::lines(1:iter, us_lda_vec, lwd = 2, col = "black")
graphics::lines(1:iter, qbc_vec, lwd = 2, col = "blue")
graphics::lines(1:iter, qbb_vec, lwd = 2, col = "purple")
graphics::lines(1:iter, cluster_vec, lwd = 2, col = "orange")
graphics::lines(1:iter, perf_results, lwd = 2, col = "green")
graphics::legend(x="bottomleft",lwd=2,cex = 0.75,legend=
c("Random Sampling","Uncertainty Sampling","Query by Committee","Query by Bagging","Min-Max Clustering",
"Given all data with labels"),
col=c("red","black","blue","purple","orange","green"))
graphics.off()
save.image(file = paste0("C:/Users/amyti/Documents/Amy - COLLEGE/THESIS/thesis-al/results/lda_", date, ".RData"))
source("C:/Users/amyti/Documents/Amy - COLLEGE/THESIS/thesis-al/main/AL_engine.R")
################################
# set up the data
date <- Sys.Date()
iter <- 50
set.seed(10)
X <- rbind(MASS::mvrnorm(100, rep(0,2), diag(2)),
MASS::mvrnorm(100, c(0.5,0), diag(2)))
y <- rep(0,200)
for (i in 1:200) {
if (X[i,1] > 0 & X[i,2] > 0) y[i] = 1
}
y <- factor(y)
# Randomly set up the unlabeled data
set.seed(10)
y_unlabeled <- y
y_unlabeled[sample(1:200,180)] <- NA
################################## Overall classifier and return methods
# Generic classifier method: X and y contain labeled points
# This is the MAIN overall classifier that will train on the data once the AL selection is completed
classifier_method <- function(X, y, ...) {
MASS::lda(X, y)
}
# Generic classifier method prediction: X contain all points to predict
# This is the MAIN overall classifier that will train on the data once the AL selection is completed
classifier_predict <- function(classifier, X, ...) {
stats::predict(classifier, X)$class
}
# QBC classifier method: X and y contain labeled points
qbc_majority <- function(X, y, committee, ...) {
tout <- vector("list",length(committee))
for (i in 1:length(committee)){
tout[[i]] <- caret::train(X,y,committee[i])
}
tout
}
# Generic error ratio: X contain all points. y are known labels (unknown to the learning algorithm)
return_method <- function(classifier, X, y, ...) {
p <- stats::predict(classifier, X)$class
length(which(p != y))/length(y)
}
# QBC error ratio: X contain all points. y are known labels (unknown to the learning algorithm)
qbc_m_return <- function(tout, X, y, committee, ...) {
p <- vector("list",length(committee))
for (i in 1:length(committee)) {
p[[i]] <- predict(tout[[i]],newdata=X)
}
# Aggregate prediction
ap <- rep(0,length(y))
for (i in 1:length(y)){
temp <- as.numeric(as.character(p[[1]][i]))
for (j in 2:length(committee)){
temp <- c(temp,as.numeric(as.character(p[[j]][i])))
}
# error checking if a value doesn't appear at all
if (is.na(as.numeric(sort(table(temp),decreasing=TRUE)[2]))) {
ap[i] <- as.numeric(names(sort(table(temp),decreasing=TRUE)[1]))
} else {
# pick one at random if there is a tie
if (as.numeric(sort(table(temp),decreasing=TRUE)[1]) == as.numeric(sort(table(temp),decreasing=TRUE)[2])){
temp <- c(0,1)
ap[i] <- sample(temp,1)
} else {
# Otherwise, insert the first one
ap[i] <- as.numeric(names(sort(table(temp),decreasing=TRUE)[1]))
}
}
}
length(which(ap != y))/length(y)
}
###################################
# The number of random unlabeled points it "streams" to the AL / RS method
# n = 0 indicates that the AL should sample from all data points
n <- 15
k <- 1
#initialize
us_lda_results <- rep(0, iter)
qbc_results <- rep(0, iter)
qbb_results <- rep(0, iter)
cluster_results <- rep(0, iter)
random_results <- rep(0, iter)
# classifier performance given all data is labeled
pred <- classifier_method(X,y)
perf_results <- rep(return_method(pred,X,y),iter)
#run the engine (average over k = 1000 random samples)
for (i in 1:k){
set.seed(10)
us_lda_results <- us_lda_results +
AL_engine(X=X, y=y, y_unlabeled=y_unlabeled, al_method = "us", classifier_method = classifier_method,
return_method = return_method, iter = iter, n = n,
classifier = "lda")
### To change the committee, you must set it in the AL_engine
set.seed(10)
qbc_results <- qbc_results +
AL_engine(X=X, y=y, y_unlabeled=y_unlabeled, al_method = "qbc", classifier_method = qbc_majority,
return_method = qbc_m_return, iter = iter, n = n,
dis = "vote_entropy", pt = 0.75)
set.seed(10)
qbb_results <- qbb_results +
AL_engine(X=X, y=y, y_unlabeled=y_unlabeled, al_method = "qbb", classifier_method = classifier_method,
return_method = return_method, iter = iter, n = n,
classifier_train=classifier_method, classifier_predict=classifier_predict,
num_class=10, r=0.75, dis = "vote_entropy")
set.seed(10)
cluster_results <- cluster_results +
AL_engine(X=X, y=y, y_unlabeled=y_unlabeled, al_method = "cluster", classifier_method = classifier_method,
return_method = return_method, iter = iter,
n = n, dis = "euclidean")
set.seed(10)
random_results <- random_results +
AL_engine(X, y, y_unlabeled, al_method = "rs", classifier_method, return_method, iter, n)
print(c("Trial ",i,"complete"))
}
# Average
us_lda_vec <- us_lda_results / k
random_vec <- random_results / k
qbc_vec <- qbc_results / k
qbb_vec <- qbb_results / k
cluster_vec <- cluster_results / k
###################################
pdf(file=paste0("C:/Users/amyti/Documents/Amy - COLLEGE/THESIS/thesis-al/results/lda_", date, ".PDF"),
height = 6, width = 10)
#plot
ymax <- max(c(us_lda_vec, random_vec, qbc_vec,cluster_vec))
graphics::plot(1:iter, random_vec, ylim = c(0, ymax), lwd = 2, type = "l",
main="AL Error Ratio with LDA Classifier", xlab="Iterations", ylab="Error", col = "red")
graphics::lines(1:iter, us_lda_vec, lwd = 2, col = "black")
graphics::lines(1:iter, qbc_vec, lwd = 2, col = "blue")
graphics::lines(1:iter, qbb_vec, lwd = 2, col = "purple")
graphics::lines(1:iter, cluster_vec, lwd = 2, col = "orange")
graphics::lines(1:iter, perf_results, lwd = 2, col = "green")
graphics::legend(x="bottomleft",lwd=2,cex = 0.75,legend=
c("Random Sampling","Uncertainty Sampling","Query by Committee","Query by Bagging","Min-Max Clustering",
"Given all data with labels"),
col=c("red","black","blue","purple","orange","green"))
graphics.off()
save.image(file = paste0("C:/Users/amyti/Documents/Amy - COLLEGE/THESIS/thesis-al/results/lda_", date, ".RData"))
qbb_results
pdf(file=paste0("C:/Users/amyti/Documents/Amy - COLLEGE/THESIS/thesis-al/results/lda_", date, ".PDF"),
height = 6, width = 10)
#plot
ymax <- max(c(us_lda_vec, random_vec, qbc_vec,cluster_vec))
graphics::plot(1:iter, perf_results, ylim = c(0, ymax), lwd = 2, type = "l",
main="AL Error Ratio with LDA Classifier", xlab="Iterations", ylab="Error", col = "green")
graphics::lines(1:iter, random_vec, lwd = 2, col = "red")
graphics::lines(1:iter, us_lda_vec, lwd = 2, col = "black")
graphics::lines(1:iter, qbc_vec, lwd = 2, col = "blue")
graphics::lines(1:iter, qbb_vec, lwd = 2, col = "purple")
graphics::lines(1:iter, cluster_vec, lwd = 2, col = "orange")
graphics::legend(x="bottomleft",lwd=2,cex = 0.75,legend=
c("Random Sampling","Uncertainty Sampling","Query by Committee","Query by Bagging","Min-Max Clustering",
"Given all data with labels"),
col=c("red","black","blue","purple","orange","green"))
graphics.off()
save.image(file = paste0("C:/Users/amyti/Documents/Amy - COLLEGE/THESIS/thesis-al/results/lda_", date, ".RData"))
pdf(file=paste0("C:/Users/amyti/Documents/Amy - COLLEGE/THESIS/thesis-al/results/lda_", date, ".PDF"),
height = 6, width = 10)
#plot
ymax <- max(c(us_lda_vec, random_vec, qbc_vec,cluster_vec))
graphics::plot(1:iter, perf_results, ylim = c(0, ymax), lwd = 2, type = "l",
main="AL Error Ratio with LDA Classifier", xlab="Iterations", ylab="Error", col = "green")
graphics::lines(1:iter, random_vec, lwd = 2, col = "red")
graphics::lines(1:iter, us_lda_vec, lwd = 2, col = "black")
graphics::lines(1:iter, qbc_vec, lwd = 2, col = "blue")
graphics::lines(1:iter, qbb_vec, lwd = 2, col = "purple")
graphics::lines(1:iter, cluster_vec, lwd = 2, col = "orange")
graphics::legend(x="bottomleft",lwd=2,cex = 0.75,legend=
c("Random Sampling","Uncertainty Sampling","Query by Committee","Query by Bagging","Min-Max Clustering",
"Given all data with labels"),
col=c("red","black","blue","purple","orange","green"))
graphics.off()
save.image(file = paste0("C:/Users/amyti/Documents/Amy - COLLEGE/THESIS/thesis-al/results/lda_", date, ".RData"))
help(plot)
###################################
pdf(file=paste0("C:/Users/amyti/Documents/Amy - COLLEGE/THESIS/thesis-al/results/lda_", date, ".PDF"),
height = 6, width = 10)
#plot
ymax <- max(c(us_lda_vec, random_vec, qbc_vec,cluster_vec))
graphics::plot(1:iter, perf_results, ylim = c(0, ymax), lwd = 2, type = "l",
main="AL Error Ratio with LDA Classifier", xlab="Iterations", ylab="Error", col = "green")
graphics::lines(1:iter, random_vec, lwd = 2, col = "red")
graphics::lines(1:iter, us_lda_vec, lwd = 2, col = "black")
graphics::lines(1:iter, qbc_vec, lwd = 2, col = "blue")
graphics::lines(1:iter, qbb_vec, lwd = 2, col = "darkturquoise")
graphics::lines(1:iter, cluster_vec, lwd = 2, col = "orange")
graphics::legend(x="bottomleft",lwd=2,cex = 0.75,legend=
c("Random Sampling","Uncertainty Sampling","Query by Committee","Query by Bagging","Min-Max Clustering",
"Given all data with labels"),
col=c("red","black","blue","darkturquoise","orange","green"))
graphics.off()
save.image(file = paste0("C:/Users/amyti/Documents/Amy - COLLEGE/THESIS/thesis-al/results/lda_", date, ".RData"))
load("C:/Users/amyti/Documents/Amy - COLLEGE/THESIS/thesis-al/results/rs_2017-02-15.RData")
setwd("C:/Users/amyti/Documents/Amy - COLLEGE/THESIS/thesis-gc/")
source("GC_engine.R")
source("dist_engine.R")
source("GC_selection.R")
library(igraph)
library(clusteval)
################################## Random sample given PDF
randomdraw <- function(n, prob){
if(sum(prob) != 1 | length(which(prob<0)) > 0) stop("Probability must be between 0 and 1")
runningsum <- 0
s <- runif(n)
for (i in 1:n){
for (j in 1:length(prob)){
runningsum <- runningsum + prob[j]
if (s[i] < runningsum){
s[i] <- j
break
}
}
}
s
}
################################## Run simulations
# Create base graph
set.seed(10)
bg <- igraph::sample_gnm(20, 100)
bg_e <- igraph::as_edgelist(bg)
# Setting parameters
gSumm <- c("cen_deg","cen_clo","cen_bet","ast",
"com_rw","com_im","com_bet","dis","eco","edh")
distf <- "l2"
msg <- rep(0,1000)
i <- 1
set.seed(i)
# g1: randomly swap 20 edges. Weight of each node is proportional to its degree
g1 <- bg
for (j in 1:20) {
idx <- sample(igraph::as_edgelist(g1),1)
g1 <- igraph::delete_edges(g1,idx)
prob <- igraph::degree(g1) / sum(igraph::degree(g1))
nv <- randomdraw(2, prob)
g1 <- igraph::add_edges(g1, nv)
}
# g2: randomly swap 100 edges. Weight of each node is proportional to its degree
g2 <- bg
for (j in 1:100) {
idx <- sample(igraph::as_edgelist(g2),1)
g2 <- igraph::delete_edges(g2,idx)
prob <- igraph::degree(g2) / sum(igraph::degree(g2))
nv <- randomdraw(2, prob)
g2 <- igraph::add_edges(g2, nv)
}
plot(g)
plot(bg)
plot(g1)
plot(g2)
plot(make_ring(g2))
bg
bg[1,3]
bg[1,10]
bg[11,10]
nv
g1[nv[1],nv[2]]
i
set.seed(i)
# g1: randomly swap 20 edges. Weight of each node is proportional to its degree
g1 <- bg
for (j in 1:20) {
idx <- sample(igraph::as_edgelist(g1),1)
g1 <- igraph::delete_edges(g1,idx)
prob <- igraph::degree(g1) / sum(igraph::degree(g1))
nv <- randomdraw(2, prob)
# make sure edges are not repeated
while (g1[nv[1],nv[2]] == 1) nv <- randomdraw(2,prob)
g1 <- igraph::add_edges(g1, nv)
}
# g2: randomly swap 100 edges. Weight of each node is proportional to its degree
g2 <- bg
for (j in 1:100) {
idx <- sample(igraph::as_edgelist(g2),1)
g2 <- igraph::delete_edges(g2,idx)
prob <- igraph::degree(g2) / sum(igraph::degree(g2))
nv <- randomdraw(2, prob)
# make sure edges are not repeated
while (g2[nv[1],nv[2]] == 1) nv <- randomdraw(2,prob)
g2 <- igraph::add_edges(g2, nv)
}
help(sample)
set.seed(i)
# g1: randomly swap 20 edges. Weight of each node is proportional to its degree
g1 <- bg
idx <- sample(igraph::as_edgelist(g1),1)
g1 <- igraph::delete_edges(g1,idx)
prob <- igraph::degree(g1) / sum(igraph::degree(g1))
nva <- sample(1,length(prob))
nvb <- randomdraw(1, prob)
nva <- sample(1,seq(1,length(prob),1))
nva
nva <- sample(1,seq(1,length(prob),1))
nva
nva <- sample(1,seq(1,length(prob),1))
nva
seq(1,length(prob),1)
help(runif)
nva <- runif(1,1,length(prob))
nva
nva <- sample(seq(1,length(prob),1),1)
nva
nva
nva
nva <- sample(seq(1,length(prob),1),1)
nva
nvb <- randomdraw(1, prob)
nvb
while (g1[nva,nvb] == 1) nv <- randomdraw(2,prob)
g1[nva,nvb]
plot(g1)
while (g1[nva,nvb] == 1) nvb <- randomdraw(1,prob)
nvb
g1[nva,nvb]
g1 <- igraph::add_edges(g1, c(nva,nvb))
g1[nva,nvb]
idx <- sample(igraph::as_edgelist(g2),1)
g2 <- igraph::delete_edges(g2,idx)
prob <- igraph::degree(g2) / sum(igraph::degree(g2))
nva <- sample(seq(1,length(prob),1),1)
nvb <- randomdraw(1, prob)
# make sure edges are not repeated
while (g2[nva,nvb] == 1) nvb <- randomdraw(1,prob)
g2 <- igraph::add_edges(g2, c(nva,nvb))
nva
nvb
setwd("C:/Users/amyti/Documents/Amy - COLLEGE/THESIS/thesis-gc/")
source("GC_engine.R")
source("dist_engine.R")
source("GC_selection.R")
library(igraph)
library(clusteval)
################################## Random sample given PDF
randomdraw <- function(n, prob){
if(sum(prob) != 1 | length(which(prob<0)) > 0) stop("Probability must be between 0 and 1")
runningsum <- 0
s <- runif(n)
for (i in 1:n){
for (j in 1:length(prob)){
runningsum <- runningsum + prob[j]
if (s[i] < runningsum){
s[i] <- j
break
}
}
}
s
}
################################## Run simulations
# Create base graph
set.seed(10)
bg <- igraph::sample_gnm(20, 100)
bg_e <- igraph::as_edgelist(bg)
# Setting parameters
gSumm <- c("cen_deg","cen_clo","cen_bet","ast",
"com_rw","com_im","com_bet","dis","eco","edh")
distf <- "l2"
msg <- rep(0,1000)
for (i in 1:1000) {
set.seed(i)
# g1: randomly swap 20 edges. Weight of each node is proportional to its degree
g1 <- bg
for (j in 1:20) {
idx <- sample(igraph::as_edgelist(g1),1)
g1 <- igraph::delete_edges(g1,idx)
prob <- igraph::degree(g1) / sum(igraph::degree(g1))
nva <- sample(seq(1,length(prob),1),1)
nvb <- randomdraw(1, prob)
# make sure edges are not repeated
while (g1[nva,nvb] == 1) nvb <- randomdraw(1,prob)
g1 <- igraph::add_edges(g1, c(nva,nvb))
}
# g2: randomly swap 100 edges. Weight of each node is proportional to its degree
g2 <- bg
for (j in 1:100) {
idx <- sample(igraph::as_edgelist(g2),1)
g2 <- igraph::delete_edges(g2,idx)
prob <- igraph::degree(g2) / sum(igraph::degree(g2))
nva <- sample(seq(1,length(prob),1),1)
nvb <- randomdraw(1, prob)
# make sure edges are not repeated
while (g2[nva,nvb] == 1) nvb <- randomdraw(1,prob)
g2 <- igraph::add_edges(g2, c(nva,nvb))
}
# Compute difference between (bg,g1), (bg,g2)
gc <- vector("list",2)
gc[[1]] <- GC_engine(g1=bg,g2=g1,gSumm=gSumm,distf=distf)
gc[[2]] <- GC_engine(g1=bg,g2=g2,gSumm=gSumm,distf=distf)
# Select the most similar graph pair
msg[i] <- GC_selection(gc = gc, base = 1)
}
# Create base graph
set.seed(10)
bg <- igraph::sample_gnm(20, 50)
bg_e <- igraph::as_edgelist(bg)
plot(bg)
setwd("C:/Users/amyti/Documents/Amy - COLLEGE/THESIS/thesis-gc/")
source("GC_engine.R")
source("dist_engine.R")
source("GC_selection.R")
library(igraph)
library(clusteval)
################################## Random sample given PDF
randomdraw <- function(n, prob){
if(sum(prob) != 1 | length(which(prob<0)) > 0) stop("Probability must be between 0 and 1")
runningsum <- 0
s <- runif(n)
for (i in 1:n){
for (j in 1:length(prob)){
runningsum <- runningsum + prob[j]
if (s[i] < runningsum){
s[i] <- j
break
}
}
}
s
}
################################## Run simulations
# Create base graph
set.seed(10)
bg <- igraph::sample_gnm(20, 50)
bg_e <- igraph::as_edgelist(bg)
# Setting parameters
gSumm <- c("cen_deg","cen_clo","cen_bet","ast",
"com_rw","com_im","com_bet","dis","eco","edh")
distf <- "l2"
msg <- rep(0,1000)
for (i in 1:1000) {
set.seed(i)
# g1: randomly swap 20 edges. Weight of each node is proportional to its degree
g1 <- bg
for (j in 1:20) {
idx <- sample(igraph::as_edgelist(g1),1)
g1 <- igraph::delete_edges(g1,idx)
prob <- igraph::degree(g1) / sum(igraph::degree(g1))
nva <- sample(seq(1,length(prob),1),1)
nvb <- randomdraw(1, prob)
# make sure edges are not repeated
while (g1[nva,nvb] == 1) nvb <- randomdraw(1,prob)
g1 <- igraph::add_edges(g1, c(nva,nvb))
}
# g2: randomly swap 100 edges. Weight of each node is proportional to its degree
g2 <- bg
for (j in 1:100) {
idx <- sample(igraph::as_edgelist(g2),1)
g2 <- igraph::delete_edges(g2,idx)
prob <- igraph::degree(g2) / sum(igraph::degree(g2))
nva <- sample(seq(1,length(prob),1),1)
nvb <- randomdraw(1, prob)
# make sure edges are not repeated
while (g2[nva,nvb] == 1) nvb <- randomdraw(1,prob)
g2 <- igraph::add_edges(g2, c(nva,nvb))
}
# Compute difference between (bg,g1), (bg,g2)
gc <- vector("list",2)
gc[[1]] <- GC_engine(g1=bg,g2=g1,gSumm=gSumm,distf=distf)
gc[[2]] <- GC_engine(g1=bg,g2=g2,gSumm=gSumm,distf=distf)
# Select the most similar graph pair
msg[i] <- GC_selection(gc = gc, base = 1)
}
p1 <- length(which(msg == 1)) / length(msg)
p2 <- length(which(msg == 2)) / length(msg)
cat("Prob. of selecting (bg,g1):",p1,"\nProb. of selecting (bg,g2):", p2)
prob
sum(prob)
