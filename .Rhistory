# n = 0 indicates that the AL should sample from all data points
n <- 15
k <- 1
#initialize
us_lda_results <- rep(0, iter)
qbc_results <- rep(0, iter)
qbb_results <- rep(0, iter)
cluster_results <- rep(0, iter)
random_results <- rep(0, iter)
# classifier performance given all data is labeled
pred <- classifier_method(X,y)
perf_results <- rep(return_method(pred,X,y),iter)
#run the engine (average over k = 1000 random samples)
for (i in 1:k){
set.seed(10)
us_lda_results <- us_lda_results +
AL_engine(X=X, y=y, y_unlabeled=y_unlabeled, al_method = "us", classifier_method = classifier_method,
return_method = return_method, iter = iter, n = n,
classifier = "lda")
### To change the committee, you must set it in the AL_engine
set.seed(10)
qbc_results <- qbc_results +
AL_engine(X=X, y=y, y_unlabeled=y_unlabeled, al_method = "qbc", classifier_method = qbc_majority,
return_method = qbc_m_return, iter = iter, n = n,
dis = "vote_entropy", pt = 0.75)
set.seed(10)
qbb_results <- qbb_results +
AL_engine(X=X, y=y, y_unlabeled=y_unlabeled, al_method = "qbb", classifier_method = classifier_method,
return_method = return_method, iter = iter, n = n,
classifier_train=classifier_method, classifier_predict=classifier_predict,
num_class=10, r=0.75, dis = "vote_entropy")
set.seed(10)
cluster_results <- cluster_results +
AL_engine(X=X, y=y, y_unlabeled=y_unlabeled, al_method = "cluster", classifier_method = classifier_method,
return_method = return_method, iter = iter,
n = n, dis = "euclidean")
set.seed(10)
random_results <- random_results +
AL_engine(X, y, y_unlabeled, al_method = "rs", classifier_method, return_method, iter, n)
print(c("Trial ",i,"complete"))
}
# Average
us_lda_vec <- us_lda_results / k
random_vec <- random_results / k
qbc_vec <- qbc_results / k
qbb_vec <- qbb_results / k
cluster_vec <- cluster_results / k
###################################
pdf(file=paste0("C:/Users/amyti/Documents/Amy - COLLEGE/THESIS/thesis-al/results/lda_", date, ".PDF"),
height = 6, width = 10)
#plot
ymax <- max(c(us_lda_vec, random_vec, qbc_vec,cluster_vec))
graphics::plot(1:iter, random_vec, ylim = c(0, ymax), lwd = 2, type = "l",
main="AL Error Ratio with LDA Classifier", xlab="Iterations", ylab="Error", col = "red")
graphics::lines(1:iter, us_lda_vec, lwd = 2, col = "black")
graphics::lines(1:iter, qbc_vec, lwd = 2, col = "blue")
graphics::lines(1:iter, qbb_vec, lwd = 2, col = "purple")
graphics::lines(1:iter, cluster_vec, lwd = 2, col = "orange")
graphics::lines(1:iter, perf_results, lwd = 2, col = "green")
graphics::legend(x="bottomleft",lwd=2,cex = 0.75,legend=
c("Random Sampling","Uncertainty Sampling","Query by Committee","Query by Bagging","Min-Max Clustering",
"Given all data with labels"),
col=c("red","black","blue","purple","orange","green"))
graphics.off()
save.image(file = paste0("C:/Users/amyti/Documents/Amy - COLLEGE/THESIS/thesis-al/results/lda_", date, ".RData"))
source("C:/Users/amyti/Documents/Amy - COLLEGE/THESIS/thesis-al/main/AL_header.R")
################################
# set up the data
date <- Sys.Date()
iter <- 50
set.seed(10)
X <- rbind(MASS::mvrnorm(100, rep(0,2), diag(2)),
MASS::mvrnorm(100, c(0.5,0), diag(2)))
y <- rep(0,200)
for (i in 1:200) {
if (X[i,1] > 0 & X[i,2] > 0) y[i] = 1
}
y <- factor(y)
# Randomly set up the unlabeled data
set.seed(10)
y_unlabeled <- y
y_unlabeled[sample(1:200,180)] <- NA
################################## Overall classifier and return methods
# Generic classifier method: X and y contain labeled points
# This is the MAIN overall classifier that will train on the data once the AL selection is completed
classifier_method <- function(X, y, ...) {
MASS::lda(X, y)
}
# Generic classifier method prediction: X contain all points to predict
# This is the MAIN overall classifier that will train on the data once the AL selection is completed
classifier_predict <- function(classifier, X, ...) {
stats::predict(classifier, X)$class
}
# QBC classifier method: X and y contain labeled points
qbc_majority <- function(X, y, committee, ...) {
tout <- vector("list",length(committee))
for (i in 1:length(committee)){
tout[[i]] <- caret::train(X,y,committee[i])
}
tout
}
# Generic error ratio: X contain all points. y are known labels (unknown to the learning algorithm)
return_method <- function(classifier, X, y, ...) {
p <- stats::predict(classifier, X)$class
length(which(p != y))/length(y)
}
# QBC error ratio: X contain all points. y are known labels (unknown to the learning algorithm)
qbc_m_return <- function(tout, X, y, committee, ...) {
p <- vector("list",length(committee))
for (i in 1:length(committee)) {
p[[i]] <- predict(tout[[i]],newdata=X)
}
# Aggregate prediction
ap <- rep(0,length(y))
for (i in 1:length(y)){
temp <- as.numeric(as.character(p[[1]][i]))
for (j in 2:length(committee)){
temp <- c(temp,as.numeric(as.character(p[[j]][i])))
}
# error checking if a value doesn't appear at all
if (is.na(as.numeric(sort(table(temp),decreasing=TRUE)[2]))) {
ap[i] <- as.numeric(names(sort(table(temp),decreasing=TRUE)[1]))
} else {
# pick one at random if there is a tie
if (as.numeric(sort(table(temp),decreasing=TRUE)[1]) == as.numeric(sort(table(temp),decreasing=TRUE)[2])){
temp <- c(0,1)
ap[i] <- sample(temp,1)
} else {
# Otherwise, insert the first one
ap[i] <- as.numeric(names(sort(table(temp),decreasing=TRUE)[1]))
}
}
}
length(which(ap != y))/length(y)
}
###################################
# The number of random unlabeled points it "streams" to the AL / RS method
# n = 0 indicates that the AL should sample from all data points
n <- 15
k <- 1
#initialize
us_lda_results <- rep(0, iter)
qbc_results <- rep(0, iter)
qbb_results <- rep(0, iter)
cluster_results <- rep(0, iter)
random_results <- rep(0, iter)
# classifier performance given all data is labeled
pred <- classifier_method(X,y)
perf_results <- rep(return_method(pred,X,y),iter)
#run the engine (average over k = 1000 random samples)
for (i in 1:k){
set.seed(10)
us_lda_results <- us_lda_results +
AL_engine(X=X, y=y, y_unlabeled=y_unlabeled, al_method = "us", classifier_method = classifier_method,
return_method = return_method, iter = iter, n = n,
classifier = "lda")
### To change the committee, you must set it in the AL_engine
set.seed(10)
qbc_results <- qbc_results +
AL_engine(X=X, y=y, y_unlabeled=y_unlabeled, al_method = "qbc", classifier_method = qbc_majority,
return_method = qbc_m_return, iter = iter, n = n,
dis = "vote_entropy", pt = 0.75)
set.seed(10)
qbb_results <- qbb_results +
AL_engine(X=X, y=y, y_unlabeled=y_unlabeled, al_method = "qbb", classifier_method = classifier_method,
return_method = return_method, iter = iter, n = n,
classifier_train=classifier_method, classifier_predict=classifier_predict,
num_class=10, r=0.75, dis = "vote_entropy")
set.seed(10)
cluster_results <- cluster_results +
AL_engine(X=X, y=y, y_unlabeled=y_unlabeled, al_method = "cluster", classifier_method = classifier_method,
return_method = return_method, iter = iter,
n = n, dis = "euclidean")
set.seed(10)
random_results <- random_results +
AL_engine(X, y, y_unlabeled, al_method = "rs", classifier_method, return_method, iter, n)
print(c("Trial ",i,"complete"))
}
# Average
us_lda_vec <- us_lda_results / k
random_vec <- random_results / k
qbc_vec <- qbc_results / k
qbb_vec <- qbb_results / k
cluster_vec <- cluster_results / k
###################################
pdf(file=paste0("C:/Users/amyti/Documents/Amy - COLLEGE/THESIS/thesis-al/results/lda_", date, ".PDF"),
height = 6, width = 10)
#plot
ymax <- max(c(us_lda_vec, random_vec, qbc_vec,cluster_vec))
graphics::plot(1:iter, random_vec, ylim = c(0, ymax), lwd = 2, type = "l",
main="AL Error Ratio with LDA Classifier", xlab="Iterations", ylab="Error", col = "red")
graphics::lines(1:iter, us_lda_vec, lwd = 2, col = "black")
graphics::lines(1:iter, qbc_vec, lwd = 2, col = "blue")
graphics::lines(1:iter, qbb_vec, lwd = 2, col = "purple")
graphics::lines(1:iter, cluster_vec, lwd = 2, col = "orange")
graphics::lines(1:iter, perf_results, lwd = 2, col = "green")
graphics::legend(x="bottomleft",lwd=2,cex = 0.75,legend=
c("Random Sampling","Uncertainty Sampling","Query by Committee","Query by Bagging","Min-Max Clustering",
"Given all data with labels"),
col=c("red","black","blue","purple","orange","green"))
graphics.off()
save.image(file = paste0("C:/Users/amyti/Documents/Amy - COLLEGE/THESIS/thesis-al/results/lda_", date, ".RData"))
source("C:/Users/amyti/Documents/Amy - COLLEGE/THESIS/thesis-al/main/AL_engine.R")
################################
# set up the data
date <- Sys.Date()
iter <- 50
set.seed(10)
X <- rbind(MASS::mvrnorm(100, rep(0,2), diag(2)),
MASS::mvrnorm(100, c(0.5,0), diag(2)))
y <- rep(0,200)
for (i in 1:200) {
if (X[i,1] > 0 & X[i,2] > 0) y[i] = 1
}
y <- factor(y)
# Randomly set up the unlabeled data
set.seed(10)
y_unlabeled <- y
y_unlabeled[sample(1:200,180)] <- NA
################################## Overall classifier and return methods
# Generic classifier method: X and y contain labeled points
# This is the MAIN overall classifier that will train on the data once the AL selection is completed
classifier_method <- function(X, y, ...) {
MASS::lda(X, y)
}
# Generic classifier method prediction: X contain all points to predict
# This is the MAIN overall classifier that will train on the data once the AL selection is completed
classifier_predict <- function(classifier, X, ...) {
stats::predict(classifier, X)$class
}
# QBC classifier method: X and y contain labeled points
qbc_majority <- function(X, y, committee, ...) {
tout <- vector("list",length(committee))
for (i in 1:length(committee)){
tout[[i]] <- caret::train(X,y,committee[i])
}
tout
}
# Generic error ratio: X contain all points. y are known labels (unknown to the learning algorithm)
return_method <- function(classifier, X, y, ...) {
p <- stats::predict(classifier, X)$class
length(which(p != y))/length(y)
}
# QBC error ratio: X contain all points. y are known labels (unknown to the learning algorithm)
qbc_m_return <- function(tout, X, y, committee, ...) {
p <- vector("list",length(committee))
for (i in 1:length(committee)) {
p[[i]] <- predict(tout[[i]],newdata=X)
}
# Aggregate prediction
ap <- rep(0,length(y))
for (i in 1:length(y)){
temp <- as.numeric(as.character(p[[1]][i]))
for (j in 2:length(committee)){
temp <- c(temp,as.numeric(as.character(p[[j]][i])))
}
# error checking if a value doesn't appear at all
if (is.na(as.numeric(sort(table(temp),decreasing=TRUE)[2]))) {
ap[i] <- as.numeric(names(sort(table(temp),decreasing=TRUE)[1]))
} else {
# pick one at random if there is a tie
if (as.numeric(sort(table(temp),decreasing=TRUE)[1]) == as.numeric(sort(table(temp),decreasing=TRUE)[2])){
temp <- c(0,1)
ap[i] <- sample(temp,1)
} else {
# Otherwise, insert the first one
ap[i] <- as.numeric(names(sort(table(temp),decreasing=TRUE)[1]))
}
}
}
length(which(ap != y))/length(y)
}
###################################
# The number of random unlabeled points it "streams" to the AL / RS method
# n = 0 indicates that the AL should sample from all data points
n <- 15
k <- 1
#initialize
us_lda_results <- rep(0, iter)
qbc_results <- rep(0, iter)
qbb_results <- rep(0, iter)
cluster_results <- rep(0, iter)
random_results <- rep(0, iter)
# classifier performance given all data is labeled
pred <- classifier_method(X,y)
perf_results <- rep(return_method(pred,X,y),iter)
#run the engine (average over k = 1000 random samples)
for (i in 1:k){
set.seed(10)
us_lda_results <- us_lda_results +
AL_engine(X=X, y=y, y_unlabeled=y_unlabeled, al_method = "us", classifier_method = classifier_method,
return_method = return_method, iter = iter, n = n,
classifier = "lda")
### To change the committee, you must set it in the AL_engine
set.seed(10)
qbc_results <- qbc_results +
AL_engine(X=X, y=y, y_unlabeled=y_unlabeled, al_method = "qbc", classifier_method = qbc_majority,
return_method = qbc_m_return, iter = iter, n = n,
dis = "vote_entropy", pt = 0.75)
set.seed(10)
qbb_results <- qbb_results +
AL_engine(X=X, y=y, y_unlabeled=y_unlabeled, al_method = "qbb", classifier_method = classifier_method,
return_method = return_method, iter = iter, n = n,
classifier_train=classifier_method, classifier_predict=classifier_predict,
num_class=10, r=0.75, dis = "vote_entropy")
set.seed(10)
cluster_results <- cluster_results +
AL_engine(X=X, y=y, y_unlabeled=y_unlabeled, al_method = "cluster", classifier_method = classifier_method,
return_method = return_method, iter = iter,
n = n, dis = "euclidean")
set.seed(10)
random_results <- random_results +
AL_engine(X, y, y_unlabeled, al_method = "rs", classifier_method, return_method, iter, n)
print(c("Trial ",i,"complete"))
}
# Average
us_lda_vec <- us_lda_results / k
random_vec <- random_results / k
qbc_vec <- qbc_results / k
qbb_vec <- qbb_results / k
cluster_vec <- cluster_results / k
###################################
pdf(file=paste0("C:/Users/amyti/Documents/Amy - COLLEGE/THESIS/thesis-al/results/lda_", date, ".PDF"),
height = 6, width = 10)
#plot
ymax <- max(c(us_lda_vec, random_vec, qbc_vec,cluster_vec))
graphics::plot(1:iter, random_vec, ylim = c(0, ymax), lwd = 2, type = "l",
main="AL Error Ratio with LDA Classifier", xlab="Iterations", ylab="Error", col = "red")
graphics::lines(1:iter, us_lda_vec, lwd = 2, col = "black")
graphics::lines(1:iter, qbc_vec, lwd = 2, col = "blue")
graphics::lines(1:iter, qbb_vec, lwd = 2, col = "purple")
graphics::lines(1:iter, cluster_vec, lwd = 2, col = "orange")
graphics::lines(1:iter, perf_results, lwd = 2, col = "green")
graphics::legend(x="bottomleft",lwd=2,cex = 0.75,legend=
c("Random Sampling","Uncertainty Sampling","Query by Committee","Query by Bagging","Min-Max Clustering",
"Given all data with labels"),
col=c("red","black","blue","purple","orange","green"))
graphics.off()
save.image(file = paste0("C:/Users/amyti/Documents/Amy - COLLEGE/THESIS/thesis-al/results/lda_", date, ".RData"))
qbb_results
pdf(file=paste0("C:/Users/amyti/Documents/Amy - COLLEGE/THESIS/thesis-al/results/lda_", date, ".PDF"),
height = 6, width = 10)
#plot
ymax <- max(c(us_lda_vec, random_vec, qbc_vec,cluster_vec))
graphics::plot(1:iter, perf_results, ylim = c(0, ymax), lwd = 2, type = "l",
main="AL Error Ratio with LDA Classifier", xlab="Iterations", ylab="Error", col = "green")
graphics::lines(1:iter, random_vec, lwd = 2, col = "red")
graphics::lines(1:iter, us_lda_vec, lwd = 2, col = "black")
graphics::lines(1:iter, qbc_vec, lwd = 2, col = "blue")
graphics::lines(1:iter, qbb_vec, lwd = 2, col = "purple")
graphics::lines(1:iter, cluster_vec, lwd = 2, col = "orange")
graphics::legend(x="bottomleft",lwd=2,cex = 0.75,legend=
c("Random Sampling","Uncertainty Sampling","Query by Committee","Query by Bagging","Min-Max Clustering",
"Given all data with labels"),
col=c("red","black","blue","purple","orange","green"))
graphics.off()
save.image(file = paste0("C:/Users/amyti/Documents/Amy - COLLEGE/THESIS/thesis-al/results/lda_", date, ".RData"))
pdf(file=paste0("C:/Users/amyti/Documents/Amy - COLLEGE/THESIS/thesis-al/results/lda_", date, ".PDF"),
height = 6, width = 10)
#plot
ymax <- max(c(us_lda_vec, random_vec, qbc_vec,cluster_vec))
graphics::plot(1:iter, perf_results, ylim = c(0, ymax), lwd = 2, type = "l",
main="AL Error Ratio with LDA Classifier", xlab="Iterations", ylab="Error", col = "green")
graphics::lines(1:iter, random_vec, lwd = 2, col = "red")
graphics::lines(1:iter, us_lda_vec, lwd = 2, col = "black")
graphics::lines(1:iter, qbc_vec, lwd = 2, col = "blue")
graphics::lines(1:iter, qbb_vec, lwd = 2, col = "purple")
graphics::lines(1:iter, cluster_vec, lwd = 2, col = "orange")
graphics::legend(x="bottomleft",lwd=2,cex = 0.75,legend=
c("Random Sampling","Uncertainty Sampling","Query by Committee","Query by Bagging","Min-Max Clustering",
"Given all data with labels"),
col=c("red","black","blue","purple","orange","green"))
graphics.off()
save.image(file = paste0("C:/Users/amyti/Documents/Amy - COLLEGE/THESIS/thesis-al/results/lda_", date, ".RData"))
help(plot)
###################################
pdf(file=paste0("C:/Users/amyti/Documents/Amy - COLLEGE/THESIS/thesis-al/results/lda_", date, ".PDF"),
height = 6, width = 10)
#plot
ymax <- max(c(us_lda_vec, random_vec, qbc_vec,cluster_vec))
graphics::plot(1:iter, perf_results, ylim = c(0, ymax), lwd = 2, type = "l",
main="AL Error Ratio with LDA Classifier", xlab="Iterations", ylab="Error", col = "green")
graphics::lines(1:iter, random_vec, lwd = 2, col = "red")
graphics::lines(1:iter, us_lda_vec, lwd = 2, col = "black")
graphics::lines(1:iter, qbc_vec, lwd = 2, col = "blue")
graphics::lines(1:iter, qbb_vec, lwd = 2, col = "darkturquoise")
graphics::lines(1:iter, cluster_vec, lwd = 2, col = "orange")
graphics::legend(x="bottomleft",lwd=2,cex = 0.75,legend=
c("Random Sampling","Uncertainty Sampling","Query by Committee","Query by Bagging","Min-Max Clustering",
"Given all data with labels"),
col=c("red","black","blue","darkturquoise","orange","green"))
graphics.off()
save.image(file = paste0("C:/Users/amyti/Documents/Amy - COLLEGE/THESIS/thesis-al/results/lda_", date, ".RData"))
load("C:/Users/amyti/Documents/Amy - COLLEGE/THESIS/thesis-al/results/rs_2017-02-15.RData")
setwd("C:/Users/amyti/Documents/Amy - COLLEGE/THESIS/thesis-gc/")
source("GC_engine.R")
source("dist_engine.R")
library(xlsx)
library(igraph)
library(clusteval) # for Jaccard index: cluster_similarity
# Read in data.
g <- 2 # number of sheets in file. Must be an even number
data <- vector("list",length(g))
for (i in 1:g) {
# First number = number of nodes in graph
data[[i]] <- as.vector(t( xlsx::read.xlsx("data.xlsx", sheetIndex = i, header = FALSE) ))[-1]
}
for (i in seq(1,g,2)) {
# g1 and g2 must have the same number of nodes!
if (data[[i]][1] != data[[i+1]][1]) {
stop(paste("g1 and g2 must have the same # of nodes. Error in sheets",i,"and",i+1))
}
# create g1 and g2 as igraphs
g1 <- igraph::make_empty_graph(n = data[[i]][1], directed = FALSE)
g1 <- igraph::add_edges(g1, data[[i]][-1])
g2 <- igraph::make_empty_graph(n = data[[i+1]][1], directed = FALSE)
g2 <- igraph::add_edges(g2, data[[i+1]][-1])
# call the engine to compute difference between g1 and g2
}
gorder(g2)
a <- distances(g1)
b <- distances(g2)
# change from matrix -> vector for distance computation (ordered by column)
# keep only 1 side of the matrix (both sides are the same)
# don't keep the values in the middle (since it's the same node)
a[a == Inf] <- 0
a <- a[upper.tri(a)] / (gorder(g1)-1)
b[b == Inf] <- 0
b <- b[upper.tri(b)] / (gorder(g2)-1)
a
b
help(upper.tri)
help(hist)
graphics::hist(igraph::degree(g1))
help(hist)
test <- graphics::hist(igraph::degree(g1))
test
graphics.off()
test <- graphics::hist(igraph::degree(g1),plot=FALSE)
test
test <- graphics::hist(igraph::degree(g2),plot=FALSE)
test
graphics::hist(igraph::degree(g2))$counts
graphics::hist(igraph::degree(g1),xlim=c(1,4))$counts
graphics::hist(igraph::degree(g1),xlim=c(0,4))$counts
0
gorder(g1)
gorder(g2)
# histograms should be on the same scale
a <- graphics::hist(igraph::degree(g1),plot=FALSE,xlim=c(0,gorder(g1)))$counts / igraph::gorder(g1)
b <- graphics::hist(igraph::degree(g2),plot=FALSE,xlim=c(0,gorder(g2)))$counts / igraph::gorder(g2)
graphics::hist(igraph::degree(g1),plot=FALSE,xlim=c(0,gorder(g1)))$counts
graphics::hist(igraph::degree(g1),plot=FALSE,xlim=c(0,7))$counts
help(hist)
graphics::hist(igraph::degree(g1),xlim=c(0,7))$counts
graphics::hist(igraph::degree(g1),breaks=c(0,7))
test <- graphics::hist(igraph::degree(g1),breaks=c(0,7))
test
test <- graphics::hist(igraph::degree(g1),breaks=seq(0,7,by=1))
test
test <- graphics::hist(igraph::degree(g1),breaks=seq(0,7,by=2))
test
graphics::hist(igraph::degree(g1),breaks=seq(0,7,by=2),plot=FALSE)
help(IQR)
help(length)
bw <- 2 * stats::IQR(igraph::degree(g1)) / igraph::degree(g1)^(1/3)
bw
range(c(0,1))
range(c(0,7))
diff( range(c(0,7)) ) / 2 * stats::IQR(igraph::degree(g1)) / igraph::degree(g1)^(1/3)
g2
degree(g2)
datatest <- c(degree(g2),0,0,2,7,7,7)
datatest
histogram(datatest,breaks="FD")
hist(datatest,breaks="FD")
test <- hist(datatest,breaks="FD")
test
iqr(datatest)
help(iqr)
IQR(datatest)
2*2/7^(1/3)
2*2 / 7^(1/3)
2*2 / (7^(1/3))
2*1 / (7^(1/3))
length(dataset)
length(datatest)
2*2 / (13^(1/3))
IQR(degrees(g2))
IQR(degree(g2))
stats::IQR(igraph::degree(g1))
igraph::degree(g1)^(1/3)
igraph::gorder(g1)^(1/3)
2 * stats::IQR(igraph::degree(g1)) / igraph::gorder(g1)^(1/3)
bw <- 2 * stats::IQR(igraph::degree(g1)) / igraph::gorder(g1)^(1/3)
a <- graphics::hist(igraph::degree(g1),plot=FALSE,breaks=seq(0,gorder(g1),by=bw))$counts / igraph::gorder(g1)
b <- graphics::hist(igraph::degree(g2),plot=FALSE,breaks=seq(0,gorder(g2),by=bw))$counts / igraph::gorder(g2)
a
b
bw
round(bw)
datatest
IQR(datatest)
length(datatest)
2 / (13^(1/3))
2*2 / (13^(1/3))
a <- graphics::hist(igraph::degree(g1),plot=FALSE,breaks=seq(0,gorder(g1),by=bw))$counts
b <- graphics::hist(igraph::degree(g2),plot=FALSE,breaks=seq(0,gorder(g2),by=bw))$counts
a
b
a <- graphics::hist(igraph::degree(g1),plot=FALSE,breaks=seq(0,gorder(g1),by=bw))
a
